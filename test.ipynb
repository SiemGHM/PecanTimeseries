{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "print(\"Cuda check\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u run.py \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./dataset/weather/ \\\n",
    "  --data_path traffic.csv \\\n",
    "  --model_id traffic_96_96 \\\n",
    "  --model iTransformer \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 172 \\\n",
    "  --dec_in 172 \\\n",
    "  --c_out 172 \\\n",
    "  --des 'Exp' \\\n",
    "  --exp_name partial_train \\\n",
    "  --itr 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u run.py \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./dataset/weather/ \\\n",
    "  --data_path weather.csv \\\n",
    "  --model_id traffic_96_96 \\\n",
    "  --model iTransformer \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 172 \\\n",
    "  --dec_in 172 \\\n",
    "  --c_out 172 \\\n",
    "  --des 'Exp' \\\n",
    "  --exp_name partial_train \\\n",
    "  --itr 1 > weather.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u run.py \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./Pecan/Data/ \\\n",
    "  --data_path 86_hourly_OT.csv \\\n",
    "  --model_id anotha \\\n",
    "  --model iTransformer \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 10 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 172 \\\n",
    "  --dec_in 172 \\\n",
    "  --c_out 172 \\\n",
    "  --patience 4\\\n",
    "  --des 'Exp' \\\n",
    "  --exp_name partial_train \\\n",
    "  --itr 1 > pec-86.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u run.py \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./Pecan/Data/ \\\n",
    "  --data_path AllPecHourly.csv \\\n",
    "  --model_id APH24 \\\n",
    "  --model iTransformer \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 347 \\\n",
    "  --dec_in 347 \\\n",
    "  --c_out 347 \\\n",
    "  --patience 8\\\n",
    "  --des 'Exp' \\\n",
    "  --exp_name partial_train \\\n",
    "  --itr 1 > ModITransformerWithDecoder-1.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /home/siem.hadish/.conda/envs/iT/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/siem.hadish/.conda/envs/iT/lib/python3.10/site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/siem.hadish/.conda/envs/iT/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/siem.hadish/.conda/envs/iT/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/siem.hadish/.conda/envs/iT/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/siem.hadish/.conda/envs/iT/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/siem.hadish/.conda/envs/iT/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/siem.hadish/.conda/envs/iT/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/siem.hadish/.conda/envs/iT/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/siem.hadish/.conda/envs/iT/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/siem.hadish/.conda/envs/iT/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "Downloading transformers-4.48.1-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, pyyaml, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.27.1 pyyaml-6.0.2 regex-2024.11.6 safetensors-0.5.2 tokenizers-0.21.0 transformers-4.48.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf in /home/siem.hadish/.conda/envs/iT/lib/python3.10/site-packages (5.29.3)\n",
      "Requirement already satisfied: google in /home/siem.hadish/.conda/envs/iT/lib/python3.10/site-packages (3.0.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/siem.hadish/.conda/envs/iT/lib/python3.10/site-packages (from google) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/siem.hadish/.conda/envs/iT/lib/python3.10/site-packages (from beautifulsoup4->google) (2.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade protobuf \n",
    "!pip install google\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda check\n",
      "True\n",
      "True\n",
      "args. use_gpu\n",
      "True\n",
      "Args in experiment:\n",
      "Namespace(is_training=1, model_id='TimeLLM', model='TimeLLM', data='custom', root_path='./Pecan/Data/', data_path='AllPecHourly.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=24, enc_in=347, dec_in=347, c_out=347, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=8, learning_rate=0.0001, des='Exp', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', exp_name='MTSF', channel_independence=False, inverse=False, class_strategy='projection', target_root_path='./data/electricity/', target_data_path='electricity.csv', efficient_training=False, use_norm=True, partial_start_index=0, llm_layers=6, task_name='long_term_forecast', llm_dim=4096, patch_len=16, stride=8, llm_model='LLAMA', prompt_domain=0)\n",
      "Use GPU: cuda:0\n",
      "Here here\n",
      "Loading checkpoint shards: 100%|█████████████████| 2/2 [03:35<00:00, 107.75s/it]\n",
      "loaded\n",
      ">>>>>>>start training : TimeLLM_TimeLLM_custom_M_ft96_sl48_ll24_pl512_dm8_nh2_el1_dl2048_df3_fctimeF_ebTrue_dtExp_projection_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "0   2016-01-01 06:00:00+00:00\n",
      "1   2016-01-01 07:00:00+00:00\n",
      "2   2016-01-01 08:00:00+00:00\n",
      "3   2016-01-01 09:00:00+00:00\n",
      "4   2016-01-01 10:00:00+00:00\n",
      "Name: date, dtype: datetime64[ns, UTC]\n",
      "datetime64[ns, UTC]\n",
      "train 1900\n",
      "1923   2016-03-21 09:00:00+00:00\n",
      "1924   2016-03-21 10:00:00+00:00\n",
      "1925   2016-03-21 11:00:00+00:00\n",
      "1926   2016-03-21 12:00:00+00:00\n",
      "1927   2016-03-21 13:00:00+00:00\n",
      "Name: date, dtype: datetime64[ns, UTC]\n",
      "datetime64[ns, UTC]\n",
      "val 266\n",
      "2212   2016-04-02 10:00:00+00:00\n",
      "2213   2016-04-02 11:00:00+00:00\n",
      "2214   2016-04-02 12:00:00+00:00\n",
      "2215   2016-04-02 13:00:00+00:00\n",
      "2216   2016-04-02 14:00:00+00:00\n",
      "Name: date, dtype: datetime64[ns, UTC]\n",
      "datetime64[ns, UTC]\n",
      "test 554\n",
      "Traceback (most recent call last):\n",
      "  File \"/l/users/siem.hadish/SiemS/iTransformer/run.py\", line 146, in <module>\n",
      "    exp.train(setting)\n",
      "  File \"/l/users/siem.hadish/SiemS/iTransformer/experiments/exp_long_term_forecasting.py\", line 141, in train\n",
      "    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
      "  File \"/home/siem.hadish/.conda/envs/iT/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/siem.hadish/.conda/envs/iT/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/l/users/siem.hadish/SiemS/iTransformer/model/TimeLLM.py\", line 199, in forward\n",
      "    dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
      "  File \"/l/users/siem.hadish/SiemS/iTransformer/model/TimeLLM.py\", line 238, in forecast\n",
      "    prompt_embeddings = self.llm_model.get_input_embeddings()(prompt.to(x_enc.device))  # (batch, prompt_token, dim)\n",
      "  File \"/home/siem.hadish/.conda/envs/iT/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/siem.hadish/.conda/envs/iT/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/siem.hadish/.conda/envs/iT/lib/python3.10/site-packages/torch/nn/modules/sparse.py\", line 190, in forward\n",
      "    return F.embedding(\n",
      "  File \"/home/siem.hadish/.conda/envs/iT/lib/python3.10/site-packages/torch/nn/functional.py\", line 2551, in embedding\n",
      "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.67 GiB. GPU 0 has a total capacity of 23.64 GiB of which 16.87 GiB is free. Including non-PyTorch memory, this process has 6.14 GiB memory in use. Of the allocated memory 5.95 GiB is allocated by PyTorch, and 10.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "!python -u run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./Pecan/Data/ \\\n",
    "  --data_path AllPecHourly.csv \\\n",
    "  --model_id TimeLLM \\\n",
    "  --model TimeLLM \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 24 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 347 \\\n",
    "  --dec_in 347 \\\n",
    "  --c_out 347 \\\n",
    "  --patience 8\\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
